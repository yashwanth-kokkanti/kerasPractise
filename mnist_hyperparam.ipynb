{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN5ZL89n4FHInKhLHUuWADs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashwanth-kokkanti/kerasPractise/blob/main/mnist_hyperparam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkZnlcLxaJoL"
      },
      "source": [
        "## Using Keras perform MNIST classification task with simple MLP network \n",
        "\n",
        "import keras\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import mnist\n",
        "import seaborn as sns\n",
        "from keras.initializers import RandomNormal\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCGd9UWEandj"
      },
      "source": [
        "%matplotlib inline \n",
        "\n",
        "## Write a function to update the plots for each epoch and error \n",
        "\n",
        "def plt_dynamic (x, vy, ty, ax, colors=['b']):\n",
        "  ax.plot(x, vy, 'b', label='Validation Loss')\n",
        "  ax.plot(x, ty, 'r', label='Train Loss')\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  fig.canvas.draw()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW9O4pi2baOm",
        "outputId": "68cbc33b-56b2-4f25-80a7-19505bfc464a"
      },
      "source": [
        "## Split Data into Train and test \n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print (\"Number of Training Examples : \", x_train.shape[0], \" and X_Train Shape \", x_train.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Training Examples :  60000  and X_Train Shape  (60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awp9i-NTcD0J",
        "outputId": "dbbf34b7-4135-4ca8-d196-d53c65e5e846"
      },
      "source": [
        "## Data is 3 Dimensional Vector, but MLP takes 1D vector. So need to convert into Vector. \n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])\n",
        "\n",
        "print (x_train[0])\n",
        "print (x_train.shape)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n",
            " 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n",
            " 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n",
            "   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n",
            "  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n",
            " 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n",
            " 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
            " 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n",
            " 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n",
            "  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0]\n",
            "(60000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knx5fQMOeXYJ",
        "outputId": "435bb21e-777a-4166-acd5-1744b930dd29"
      },
      "source": [
        "## Need to normalise all Data . == (X - Xmin ) / (Xmax - Xmin) = X/255 ## (Xmax = 255 and Xmin = 0)\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255 \n",
        "\n",
        "print (x_train[0])\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
            " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
            " 0.96862745 0.49803922 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
            " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.19215686\n",
            " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
            " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
            " 0.96862745 0.94509804 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
            " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.04313725\n",
            " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.1372549  0.94509804\n",
            " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
            " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
            " 0.58823529 0.10588235 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
            " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.15294118 0.58039216\n",
            " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.07058824 0.67058824\n",
            " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
            " 0.31372549 0.03529412 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.53333333 0.99215686\n",
            " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs9WnQewfJ_k",
        "outputId": "aacaf395-e7ed-404b-a2d4-e14c48442dd0"
      },
      "source": [
        "## Now Convert Output Classes from number to 10 dimensional vector \n",
        "\n",
        "y_train = np_utils.to_categorical(y_train, 10)\n",
        "y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "print (y_train[0])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jT_B7bgfoRK"
      },
      "source": [
        "## Build Model with SoftMax Classifier \n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkW8ZlfrgCgC"
      },
      "source": [
        "## Initialize Model Params \n",
        "\n",
        "output_dim = 10 \n",
        "input_dim = x_train.shape[1]\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 12\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOTHNxjmgO47"
      },
      "source": [
        "## Build model \n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(output_dim, input_dim=input_dim, activation='softmax'))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcEbDzoEgroP",
        "outputId": "89167a71-f012-4187-e006-efbce42c1655"
      },
      "source": [
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 1.2824 - accuracy: 0.7040 - val_loss: 0.8073 - val_accuracy: 0.8342\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.7108 - accuracy: 0.8432 - val_loss: 0.6030 - val_accuracy: 0.8631\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.5824 - accuracy: 0.8610 - val_loss: 0.5218 - val_accuracy: 0.8765\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.5214 - accuracy: 0.8706 - val_loss: 0.4763 - val_accuracy: 0.8819\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4845 - accuracy: 0.8765 - val_loss: 0.4470 - val_accuracy: 0.8870\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4591 - accuracy: 0.8808 - val_loss: 0.4260 - val_accuracy: 0.8901\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4403 - accuracy: 0.8838 - val_loss: 0.4100 - val_accuracy: 0.8937\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4256 - accuracy: 0.8862 - val_loss: 0.3973 - val_accuracy: 0.8962\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4139 - accuracy: 0.8887 - val_loss: 0.3873 - val_accuracy: 0.8983\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.4040 - accuracy: 0.8910 - val_loss: 0.3791 - val_accuracy: 0.8995\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3958 - accuracy: 0.8927 - val_loss: 0.3716 - val_accuracy: 0.9008\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.3886 - accuracy: 0.8941 - val_loss: 0.3654 - val_accuracy: 0.9015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsCzL9jghOhn",
        "outputId": "afc97042-440c-4ce5-ee2f-7fb1383c80ab"
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print (\"Test Score \", score[0])\n",
        "print (\"Test accuracy \", score[1])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Score  0.36544254422187805\n",
            "Test accuracy  0.9014999866485596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "X9_w8zmRiNTE",
        "outputId": "2e58307f-2968-4691-9010-59ee3eacb178"
      },
      "source": [
        "fig, ax = plt.subplots(1, 1)\n",
        "ax.set_xlabel('epoch') \n",
        "ax.set_ylabel(\"Categorical CrossEntropy Loss\")\n",
        "\n",
        "x = list(range(1, epochs+1))\n",
        "vy = history.history['val_loss']\n",
        "ty = history.history['loss']\n",
        "\n",
        "plt_dynamic(x, vy, ty, ax)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5oeg1hJi34N",
        "outputId": "1efe6135-8b3c-4fd3-c380-fa93247815c8"
      },
      "source": [
        "## Build Model with MLP + SGD Optimizer + Sigmoid Activation \n",
        "\n",
        "model_sig = Sequential()\n",
        "model_sig.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)  ))\n",
        "model_sig.add(Dense(128, activation='sigmoid'))\n",
        "model_sig.add(Dense(output_dim, activation='softmax'))\n",
        "\n",
        "model_sig.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_50\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_157 (Dense)            (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_158 (Dense)            (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dense_159 (Dense)            (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 468,874\n",
            "Trainable params: 468,874\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pWlOW44FoHM",
        "outputId": "3a5dd0e2-f04c-4443-a584-365a14bffc82"
      },
      "source": [
        "model_sig.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model_sig.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 2.2743 - accuracy: 0.2407 - val_loss: 2.2223 - val_accuracy: 0.2666\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 2.1798 - accuracy: 0.4349 - val_loss: 2.1268 - val_accuracy: 0.5754\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 2.0675 - accuracy: 0.5739 - val_loss: 1.9905 - val_accuracy: 0.6056\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 1.9069 - accuracy: 0.6277 - val_loss: 1.7995 - val_accuracy: 0.6458\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 1.6962 - accuracy: 0.6675 - val_loss: 1.5681 - val_accuracy: 0.6984\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 1.4640 - accuracy: 0.7048 - val_loss: 1.3363 - val_accuracy: 0.7263\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 1.2511 - accuracy: 0.7362 - val_loss: 1.1421 - val_accuracy: 0.7525\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 1.0803 - accuracy: 0.7611 - val_loss: 0.9933 - val_accuracy: 0.7745\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.9507 - accuracy: 0.7818 - val_loss: 0.8816 - val_accuracy: 0.7925\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.8524 - accuracy: 0.7992 - val_loss: 0.7949 - val_accuracy: 0.8104\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.7762 - accuracy: 0.8127 - val_loss: 0.7283 - val_accuracy: 0.8234\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.7160 - accuracy: 0.8235 - val_loss: 0.6747 - val_accuracy: 0.8333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "VC4_R19wHNnd",
        "outputId": "5f11cd5b-aa7c-441f-a7fb-52d3e6a00d51"
      },
      "source": [
        "score = model_sig.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print (\"Test Score :\", score[0])\n",
        "print (\"Test accuracy :\", score[1])\n",
        "\n",
        "fig, ax = plt.subplots(1,1)\n",
        "ax.set_xlabel('epoch')\n",
        "ax.set_ylabel('Categorical CrossEntropy Loss ')\n",
        "\n",
        "x = list(range(1, epochs+1))\n",
        "\n",
        "vy = history.history['val_loss']\n",
        "ty = history.history['loss']\n",
        "\n",
        "plt_dynamic(x, vy, ty, ax)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Score : 0.6747231483459473\n",
            "Test accuracy : 0.833299994468689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz9egQpWIToZ",
        "outputId": "21e46d59-7aa9-4760-d1af-ba8418e9dcb1"
      },
      "source": [
        "## MLP + Sigmoid + ADAM \n",
        "\n",
        "model_adm = Sequential()\n",
        "model_adm.add(Dense(512, activation='sigmoid', input_shape=(input_dim, )))\n",
        "model_adm.add(Dense(256, activation='sigmoid'))\n",
        "model_adm.add(Dense(128, activation='sigmoid'))\n",
        "model_adm.add(Dense(output_dim, activation='softmax'))\n",
        "\n",
        "\n",
        "model_adm.summary()\n",
        "\n",
        "model_adm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model_adm.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_51\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_160 (Dense)            (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_161 (Dense)            (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_162 (Dense)            (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_163 (Dense)            (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 567,434\n",
            "Trainable params: 567,434\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.6326 - accuracy: 0.8170 - val_loss: 0.2443 - val_accuracy: 0.9290\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.2019 - accuracy: 0.9406 - val_loss: 0.1623 - val_accuracy: 0.9502\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.1419 - accuracy: 0.9588 - val_loss: 0.1351 - val_accuracy: 0.9610\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.1079 - accuracy: 0.9681 - val_loss: 0.1120 - val_accuracy: 0.9642\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.0841 - accuracy: 0.9753 - val_loss: 0.0910 - val_accuracy: 0.9710\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.0654 - accuracy: 0.9803 - val_loss: 0.0986 - val_accuracy: 0.9687\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0531 - accuracy: 0.9834 - val_loss: 0.0710 - val_accuracy: 0.9775\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0410 - accuracy: 0.9877 - val_loss: 0.0784 - val_accuracy: 0.9763\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.0347 - accuracy: 0.9898 - val_loss: 0.0711 - val_accuracy: 0.9774\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.0276 - accuracy: 0.9918 - val_loss: 0.0740 - val_accuracy: 0.9786\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.0213 - accuracy: 0.9940 - val_loss: 0.0753 - val_accuracy: 0.9787\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.0195 - accuracy: 0.9941 - val_loss: 0.0707 - val_accuracy: 0.9803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCX12AyRNjLQ",
        "outputId": "d5d4f141-e173-4834-d14f-f6c7f07c4616"
      },
      "source": [
        " ## MLP + Relu + SGD \n",
        "\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "\n",
        "model_adm = Sequential()\n",
        "model_adm.add(Dense(512, activation='relu', input_shape=(input_dim, )))\n",
        "model_adm.add(Dense(256, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.06, seed=None)))\n",
        "model_adm.add(Dense(128, activation='relu'))\n",
        "##model.add(Dropout(0.3))\n",
        "model_adm.add(Dense(64, activation='relu'))\n",
        "model_adm.add(Dense(32, activation='relu'))\n",
        "model_adm.add(Dense(16, activation='relu'))\n",
        "##model.add(BatchNormalization())\n",
        "model_adm.add(Dense(8, activation='relu'))\n",
        "model_adm.add(Dense(output_dim, activation='softmax'))\n",
        "\n",
        "\n",
        "model_adm.summary()\n",
        "\n",
        "model_adm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model_adm.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_52\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_164 (Dense)            (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_165 (Dense)            (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_166 (Dense)            (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_167 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_168 (Dense)            (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_169 (Dense)            (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_170 (Dense)            (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_171 (Dense)            (None, 10)                90        \n",
            "=================================================================\n",
            "Total params: 577,234\n",
            "Trainable params: 577,234\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.6234 - accuracy: 0.7829 - val_loss: 0.1809 - val_accuracy: 0.9547\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.1296 - accuracy: 0.9660 - val_loss: 0.1243 - val_accuracy: 0.9684\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0842 - accuracy: 0.9767 - val_loss: 0.1195 - val_accuracy: 0.9686\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.0623 - accuracy: 0.9826 - val_loss: 0.0990 - val_accuracy: 0.9759\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0457 - accuracy: 0.9870 - val_loss: 0.1038 - val_accuracy: 0.9748\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.0349 - accuracy: 0.9898 - val_loss: 0.1023 - val_accuracy: 0.9776\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0343 - accuracy: 0.9897 - val_loss: 0.0957 - val_accuracy: 0.9782\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0285 - accuracy: 0.9916 - val_loss: 0.0970 - val_accuracy: 0.9797\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0229 - accuracy: 0.9934 - val_loss: 0.1214 - val_accuracy: 0.9753\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0226 - accuracy: 0.9934 - val_loss: 0.1211 - val_accuracy: 0.9761\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0212 - accuracy: 0.9939 - val_loss: 0.1011 - val_accuracy: 0.9793\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0169 - accuracy: 0.9949 - val_loss: 0.1204 - val_accuracy: 0.9791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AOrxFXXO_Jr",
        "outputId": "629a8748-9aa1-4311-c3d3-b98c62695ea0"
      },
      "source": [
        " ## MLP + Relu + SGD + BatchNOrmalization + Dropout \n",
        "\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "\n",
        "model_adm = Sequential()\n",
        "model_adm.add(Dense(512, activation='relu', input_shape=(input_dim, )))\n",
        "model_adm.add(Dense(256, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.06, seed=None)))\n",
        "model_adm.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.08, seed=None)))\n",
        "model.add(Dropout(0.5))\n",
        "model_adm.add(Dense(64, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.10, seed=None)))\n",
        "model_adm.add(Dense(32, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)))\n",
        "model.add(Dropout(0.3))\n",
        "model_adm.add(Dense(16, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.25, seed=None)))\n",
        "model.add(BatchNormalization())\n",
        "model_adm.add(Dense(8, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.5, seed=None)))\n",
        "model_adm.add(Dense(output_dim, activation='softmax'))\n",
        "\n",
        "\n",
        "model_adm.summary()\n",
        "\n",
        "model_adm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model_adm.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_53\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_172 (Dense)            (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_173 (Dense)            (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_174 (Dense)            (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_175 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_176 (Dense)            (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_177 (Dense)            (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_178 (Dense)            (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_179 (Dense)            (None, 10)                90        \n",
            "=================================================================\n",
            "Total params: 577,234\n",
            "Trainable params: 577,234\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.4312 - accuracy: 0.8676 - val_loss: 0.1620 - val_accuracy: 0.9575\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.1263 - accuracy: 0.9664 - val_loss: 0.1200 - val_accuracy: 0.9686\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.0823 - accuracy: 0.9777 - val_loss: 0.1147 - val_accuracy: 0.9687\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0626 - accuracy: 0.9825 - val_loss: 0.1069 - val_accuracy: 0.9734\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0485 - accuracy: 0.9860 - val_loss: 0.0958 - val_accuracy: 0.9755\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.0411 - accuracy: 0.9876 - val_loss: 0.0953 - val_accuracy: 0.9754\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0328 - accuracy: 0.9905 - val_loss: 0.0996 - val_accuracy: 0.9782\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.0275 - accuracy: 0.9924 - val_loss: 0.0990 - val_accuracy: 0.9773\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0288 - accuracy: 0.9918 - val_loss: 0.1073 - val_accuracy: 0.9757\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.0275 - accuracy: 0.9923 - val_loss: 0.1025 - val_accuracy: 0.9770\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.0177 - accuracy: 0.9947 - val_loss: 0.1023 - val_accuracy: 0.9776\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.0179 - accuracy: 0.9944 - val_loss: 0.1020 - val_accuracy: 0.9804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKchR582P2Cp"
      },
      "source": [
        "## Keras Hyper Param Tuning using SKLearn Wrapper . \n",
        "  \n",
        "from keras.optimizers import Adam, RMSprop, SGD\n",
        "\n",
        "def hyper_param(activ):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(512, activation=activ, input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
        "  model.add(Dense(128, activation=activ, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
        "  model.add(Dense(output_dim, activation='softmax'))\n",
        "  \n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], )\n",
        "  return model\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qra9sVdubf2S",
        "outputId": "167dfbc3-d519-4ce6-a2d2-6e8e4f8a565f"
      },
      "source": [
        "activ = ['sigmoid', 'relu', ]\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model = KerasClassifier(build_fn=hyper_param)\n",
        "\n",
        "##param_grid = dict(activ=activ)\n",
        "param_grid = {'activ':['sigmoid', 'relu'], 'epochs':[12, ], }\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
        "grid_result = grid.fit(x_train, y_train)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.3965 - accuracy: 0.8898\n",
            "Epoch 2/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1648 - accuracy: 0.9500\n",
            "Epoch 3/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1089 - accuracy: 0.9669\n",
            "Epoch 4/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0761 - accuracy: 0.9772\n",
            "Epoch 5/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0559 - accuracy: 0.9829\n",
            "Epoch 6/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0406 - accuracy: 0.9877\n",
            "Epoch 7/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0297 - accuracy: 0.9911\n",
            "Epoch 8/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0216 - accuracy: 0.9935\n",
            "Epoch 9/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0168 - accuracy: 0.9949\n",
            "Epoch 10/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0138 - accuracy: 0.9955\n",
            "Epoch 11/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0098 - accuracy: 0.9974\n",
            "Epoch 12/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0081 - accuracy: 0.9975\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.0908 - accuracy: 0.9789\n",
            "Epoch 1/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.3866 - accuracy: 0.8912\n",
            "Epoch 2/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1664 - accuracy: 0.9490\n",
            "Epoch 3/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1115 - accuracy: 0.9663\n",
            "Epoch 4/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0781 - accuracy: 0.9765\n",
            "Epoch 5/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0566 - accuracy: 0.9826\n",
            "Epoch 6/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0429 - accuracy: 0.9863\n",
            "Epoch 7/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0326 - accuracy: 0.9906\n",
            "Epoch 8/12\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0238 - accuracy: 0.9930\n",
            "Epoch 9/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0177 - accuracy: 0.9949\n",
            "Epoch 10/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0135 - accuracy: 0.9963\n",
            "Epoch 11/12\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0109 - accuracy: 0.9967\n",
            "Epoch 12/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0090 - accuracy: 0.9974\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.0882 - accuracy: 0.9768\n",
            "Epoch 1/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.3931 - accuracy: 0.8892\n",
            "Epoch 2/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1673 - accuracy: 0.9493\n",
            "Epoch 3/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1118 - accuracy: 0.9667\n",
            "Epoch 4/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0782 - accuracy: 0.9754\n",
            "Epoch 5/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0571 - accuracy: 0.9825\n",
            "Epoch 6/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0409 - accuracy: 0.9877\n",
            "Epoch 7/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0302 - accuracy: 0.9906\n",
            "Epoch 8/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0241 - accuracy: 0.9925\n",
            "Epoch 9/12\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0161 - accuracy: 0.9956\n",
            "Epoch 10/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0134 - accuracy: 0.9961\n",
            "Epoch 11/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0106 - accuracy: 0.9969\n",
            "Epoch 12/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0090 - accuracy: 0.9973\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1008 - accuracy: 0.9737\n",
            "Epoch 1/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.3836 - accuracy: 0.8929\n",
            "Epoch 2/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1633 - accuracy: 0.9509\n",
            "Epoch 3/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1060 - accuracy: 0.9677\n",
            "Epoch 4/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0746 - accuracy: 0.9775\n",
            "Epoch 5/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0552 - accuracy: 0.9833\n",
            "Epoch 6/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0402 - accuracy: 0.9879\n",
            "Epoch 7/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0282 - accuracy: 0.9918\n",
            "Epoch 8/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0212 - accuracy: 0.9941\n",
            "Epoch 9/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0164 - accuracy: 0.9951\n",
            "Epoch 10/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0131 - accuracy: 0.9961\n",
            "Epoch 11/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0088 - accuracy: 0.9973\n",
            "Epoch 12/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0078 - accuracy: 0.9977\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1068 - accuracy: 0.9743\n",
            "Epoch 1/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.3890 - accuracy: 0.8917\n",
            "Epoch 2/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1662 - accuracy: 0.9494\n",
            "Epoch 3/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1097 - accuracy: 0.9669\n",
            "Epoch 4/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0755 - accuracy: 0.9766\n",
            "Epoch 5/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0550 - accuracy: 0.9833\n",
            "Epoch 6/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0405 - accuracy: 0.9873\n",
            "Epoch 7/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0286 - accuracy: 0.9917\n",
            "Epoch 8/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0211 - accuracy: 0.9939\n",
            "Epoch 9/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0168 - accuracy: 0.9950\n",
            "Epoch 10/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0131 - accuracy: 0.9961\n",
            "Epoch 11/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0104 - accuracy: 0.9969\n",
            "Epoch 12/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0080 - accuracy: 0.9975\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.0905 - accuracy: 0.9781\n",
            "Epoch 1/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2128 - accuracy: 0.9355\n",
            "Epoch 2/12\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0855 - accuracy: 0.9738\n",
            "Epoch 3/12\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0573 - accuracy: 0.9816\n",
            "Epoch 4/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0414 - accuracy: 0.9858\n",
            "Epoch 5/12\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0328 - accuracy: 0.9893\n",
            "Epoch 6/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0262 - accuracy: 0.9912\n",
            "Epoch 7/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0199 - accuracy: 0.9942\n",
            "Epoch 8/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0197 - accuracy: 0.9936\n",
            "Epoch 9/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0191 - accuracy: 0.9938\n",
            "Epoch 10/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0161 - accuracy: 0.9947\n",
            "Epoch 11/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0140 - accuracy: 0.9957\n",
            "Epoch 12/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0149 - accuracy: 0.9952\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1065 - accuracy: 0.9787\n",
            "Epoch 1/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2086 - accuracy: 0.9377\n",
            "Epoch 2/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0847 - accuracy: 0.9732\n",
            "Epoch 3/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0576 - accuracy: 0.9818\n",
            "Epoch 4/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0435 - accuracy: 0.9851\n",
            "Epoch 5/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0332 - accuracy: 0.9890\n",
            "Epoch 6/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0267 - accuracy: 0.9911\n",
            "Epoch 7/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0277 - accuracy: 0.9910\n",
            "Epoch 8/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0198 - accuracy: 0.9936\n",
            "Epoch 9/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0152 - accuracy: 0.9951\n",
            "Epoch 10/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0177 - accuracy: 0.9942\n",
            "Epoch 11/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0156 - accuracy: 0.9952\n",
            "Epoch 12/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0144 - accuracy: 0.9953\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.0949 - accuracy: 0.9788\n",
            "Epoch 1/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2097 - accuracy: 0.9366\n",
            "Epoch 2/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0852 - accuracy: 0.9728\n",
            "Epoch 3/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0542 - accuracy: 0.9824\n",
            "Epoch 4/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0423 - accuracy: 0.9858\n",
            "Epoch 5/12\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0321 - accuracy: 0.9895\n",
            "Epoch 6/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0264 - accuracy: 0.9911\n",
            "Epoch 7/12\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0246 - accuracy: 0.9919\n",
            "Epoch 8/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0183 - accuracy: 0.9941\n",
            "Epoch 9/12\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0167 - accuracy: 0.9945\n",
            "Epoch 10/12\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0171 - accuracy: 0.9945\n",
            "Epoch 11/12\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0150 - accuracy: 0.9954\n",
            "Epoch 12/12\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0126 - accuracy: 0.9959\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1467 - accuracy: 0.9743\n",
            "Epoch 1/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2060 - accuracy: 0.9376\n",
            "Epoch 2/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0822 - accuracy: 0.9738\n",
            "Epoch 3/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0537 - accuracy: 0.9828\n",
            "Epoch 4/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0411 - accuracy: 0.9865\n",
            "Epoch 5/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0321 - accuracy: 0.9894\n",
            "Epoch 6/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0235 - accuracy: 0.9921\n",
            "Epoch 7/12\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0213 - accuracy: 0.9934\n",
            "Epoch 8/12\n",
            "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0227 - accuracy: 0.9927\n",
            "Epoch 9/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0145 - accuracy: 0.9954\n",
            "Epoch 10/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0157 - accuracy: 0.9949\n",
            "Epoch 11/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0155 - accuracy: 0.9955\n",
            "Epoch 12/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0160 - accuracy: 0.9954\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1268 - accuracy: 0.9750\n",
            "Epoch 1/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.2100 - accuracy: 0.9365\n",
            "Epoch 2/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0849 - accuracy: 0.9731\n",
            "Epoch 3/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0553 - accuracy: 0.9820\n",
            "Epoch 4/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0425 - accuracy: 0.9870\n",
            "Epoch 5/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0312 - accuracy: 0.9899\n",
            "Epoch 6/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0268 - accuracy: 0.9918\n",
            "Epoch 7/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0221 - accuracy: 0.9927\n",
            "Epoch 8/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0201 - accuracy: 0.9935\n",
            "Epoch 9/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0162 - accuracy: 0.9947\n",
            "Epoch 10/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0163 - accuracy: 0.9944\n",
            "Epoch 11/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0173 - accuracy: 0.9945\n",
            "Epoch 12/12\n",
            "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0132 - accuracy: 0.9959\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1036 - accuracy: 0.9797\n",
            "Epoch 1/12\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1891 - accuracy: 0.9420\n",
            "Epoch 2/12\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0780 - accuracy: 0.9759\n",
            "Epoch 3/12\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0556 - accuracy: 0.9822\n",
            "Epoch 4/12\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0405 - accuracy: 0.9874\n",
            "Epoch 5/12\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0325 - accuracy: 0.9893\n",
            "Epoch 6/12\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0262 - accuracy: 0.9915\n",
            "Epoch 7/12\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0220 - accuracy: 0.9927\n",
            "Epoch 8/12\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0190 - accuracy: 0.9937\n",
            "Epoch 9/12\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0182 - accuracy: 0.9941\n",
            "Epoch 10/12\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0163 - accuracy: 0.9948\n",
            "Epoch 11/12\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0161 - accuracy: 0.9947\n",
            "Epoch 12/12\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0135 - accuracy: 0.9959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-jyptnudwd5",
        "outputId": "a01eb50f-03ae-4c71-f7ed-59354de4d86f"
      },
      "source": [
        "print (\"Best Model : \", grid_result.best_score_, grid_result.best_params_)\n",
        "mns = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, std, param in zip(mns, stds, params):\n",
        "  print (mean, std, param)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Model :  0.9773333311080933 {'activ': 'relu', 'epochs': 12}\n",
            "0.9763833284378052 0.002032093732623829 {'activ': 'sigmoid', 'epochs': 12}\n",
            "0.9773333311080933 0.0022154592297583685 {'activ': 'relu', 'epochs': 12}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMtSxXMIx6Nv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}